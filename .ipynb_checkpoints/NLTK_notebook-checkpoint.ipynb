{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizer\n",
    "- It converts paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The limits of your language are the limits of your world.\\\"-Ludwig Wittgenstein  Our English learner's have a strong support system at home that begs for more resources.  Many times our parents are learning to read and speak English along side of their children.  Sometimes this creates barriers for parents to be able to help their child learn phonetics, letter recognition, and other reading skills.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The limits of your language are the limits of your world.', '\"-Ludwig Wittgenstein  Our English learner\\'s have a strong support system at home that begs for more resources.', 'Many times our parents are learning to read and speak English along side of their children.', 'Sometimes this creates barriers for parents to be able to help their child learn phonetics, letter recognition, and other reading skills.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "p = sent_tokenize(text);\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"-Ludwig Wittgenstein  Our English learner\\'s have a strong support system at home that begs for more resources.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The limits of your language are the limits of your world.',\n",
       " '\"-Ludwig Wittgenstein  Our English learner\\'s have a strong support system at home that begs for more resources.',\n",
       " 'Many times our parents are learning to read and speak English along side of their children.',\n",
       " 'Sometimes this creates barriers for parents to be able to help their child learn phonetics, letter recognition, and other reading skills.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "array = tokenizer.tokenize(text)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing using english.pickle, it contains data from various fiction which is used to tokenize english paragraph into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's used to tokenize sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize;\n",
    "\n",
    "words = word_tokenize(array[0]);\n",
    "words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wo', \"n't\", 'be', 'able', 'to', 'come', 'today']\n",
      "['I', 'wo', \"n't\", 'be', 'able', 'to', 'come', 'today']\n",
      "['I', 'won', \"'\", 't', 'be', 'able', 'to', 'come', 'today']\n"
     ]
    }
   ],
   "source": [
    "#Various ways to perfrom word tokenization\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer_2 = TreebankWordTokenizer();\n",
    "tokenizer_3 = WordPunctTokenizer();\n",
    "\n",
    "sent = \"I won't be able to come today\"\n",
    "\n",
    "result1 = word_tokenize(sent);\n",
    "print(result1);\n",
    "\n",
    "result2 = tokenizer_2.tokenize(sent);\n",
    "print(result2);\n",
    "\n",
    "result3 = tokenizer_3.tokenize(sent);\n",
    "print(result3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_tokenize, TreebankWordTokenizer and WordPunctTokenizer works similarly in most of the cases.\n",
    "- Difference comes when sentences have words like won't, can't.\n",
    "- As we can see word_tokenize and TreebankWordTokenizer works same that means they are breaking that word.\n",
    "- But WordPunctTokenizer breaking from ' and including ' also in result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regexp Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Regular Expression to perfor word tokenization\n",
    "- Example in above sentence we see that it was breaking word like won't into two parts, so we can use regular expression to tell tokenizer that don't break such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"won't\", 'be', 'in', 'class', 'today', 'because', 'I', \"can't\", 'drive']\n",
      "['I', 'wo', \"n't\", 'be', 'in', 'class', 'today', ',', 'because', 'I', 'ca', \"n't\", 'drive']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize;\n",
    "\n",
    "sent = \"I won't be in class today, because I can't drive\";\n",
    "\n",
    "result = regexp_tokenize(sent, \"[\\w']+\");\n",
    "print(result)\n",
    "\n",
    "#comparing it with word_tokenize\n",
    "result2 = word_tokenize(sent);\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here we can see that it's not breaking won't into parts while tokenizing because we are passing regular expression which says preserve words having ' in them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stop words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = stopwords.words('english');\n",
    "print(STOP_WORDS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = [word for word in text.split() if word.lower() not in STOP_WORDS];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before removing stopwords: \n",
      "The limits of your language are the limits of your world.\"-Ludwig Wittgenstein  Our English learner's have a strong support system at home that begs for more resources.  Many times our parents are learning to read and speak English along side of their children.  Sometimes this creates barriers for parents to be able to help their child learn phonetics, letter recognition, and other reading skills.\n",
      "--------------------------------------------------\n",
      "Text after removing stopwords: \n",
      "limits language limits world.\"-Ludwig Wittgenstein English learner's strong support system home begs resources. Many times parents learning read speak English along side children. Sometimes creates barriers parents able help child learn phonetics, letter recognition, reading skills.\n"
     ]
    }
   ],
   "source": [
    "print('Text before removing stopwords: ');\n",
    "print(text)\n",
    "\n",
    "print('-'*50);\n",
    "\n",
    "print('Text after removing stopwords: ');\n",
    "print(' '.join(clean_text));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Wordnet</h2>\n",
    "- It's a dictionary of English with some advance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>synsets</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's a synonym set, which is a collection of synonym words\n",
    "- wordent.synsets(w) return different words in different context of word w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('weapon.n.01'), Synset('weapon.n.02')]\n",
      "Related words:  ['weapon', 'arm', 'weapon_system']\n",
      "Definition of weapon:  any instrument or instrumentality used in fighting or hunting\n",
      "Parts of Speech of weapon:  n\n",
      "['he was licensed to carry a weapon']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "word1 = \"weapon\";\n",
    "\n",
    "synArray = wordnet.synsets(word1);\n",
    "\n",
    "print(synArray); #it has words with 2 contexts the visible names are called lemma code name\n",
    "#the second letter after dot represents part of speech of word\n",
    "\n",
    "#let's pick words present in firs context\n",
    "woi = synArray[0];\n",
    "\n",
    "#lemma_names() gives words related to this context\n",
    "print(\"Related words: \", woi.lemma_names())\n",
    "\n",
    "#it gives definition of the obtained synonym word\n",
    "print(\"Definition of weapon: \",woi.definition())\n",
    "\n",
    "#Obtianing part of speech \n",
    "print(\"Parts of Speech of weapon: \", woi.pos())\n",
    "\n",
    "#examples() method return sentences containing the word 'weapon'\n",
    "print(woi.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>hypernyms and hyponyms</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hypernyms return more general words for given word: Parent word\n",
    "- hyponyms return more specific words for given word: Child word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('instrument.n.01')]\n",
      "a device that requires skill for proper use\n"
     ]
    }
   ],
   "source": [
    "#example for hypernyms\n",
    "\n",
    "hypernyms = woi.hypernyms();\n",
    "print(hypernyms)\n",
    "\n",
    "#obtaining definintion of obtained context;\n",
    "print(hypernyms[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bow.n.04'), Synset('bow_and_arrow.n.01'), Synset('brass_knucks.n.01'), Synset('fire_ship.n.01'), Synset('flamethrower.n.01'), Synset('greek_fire.n.01'), Synset('gun.n.01'), Synset('knife.n.02'), Synset('light_arm.n.01'), Synset('missile.n.01'), Synset('pike.n.04'), Synset('projectile.n.01'), Synset('slasher.n.02'), Synset('sling.n.04'), Synset('spear.n.01'), Synset('stun_gun.n.01'), Synset('sword.n.01'), Synset('tomahawk.n.01'), Synset('weapon_of_mass_destruction.n.01')]\n",
      "--------------------------------------------------\n",
      "definition:  a weapon for shooting arrows, composed of a curved piece of resilient wood with a taut cord to propel the arrow\n",
      "lemma names:  ['bow']\n"
     ]
    }
   ],
   "source": [
    "#example for hypnonyms\n",
    "\n",
    "hyponyms = woi.hyponyms();\n",
    "print(hyponyms) #it will return all the specific contexts related to weapons\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "#fetch first synset\n",
    "first_hyponym = hyponyms[0];\n",
    "\n",
    "#obtianing definition\n",
    "print(\"definition: \", first_hyponym.definition())\n",
    "\n",
    "#obtaining lemma names\n",
    "print(\"lemma names: \", first_hyponym.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Lemmas, Synonyms, Antonyms</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('win.n.01'), Synset('winnings.n.01'), Synset('win.v.01'), Synset('acquire.v.05'), Synset('gain.v.05'), Synset('succeed.v.01')]\n"
     ]
    }
   ],
   "source": [
    "word = 'win';\n",
    "\n",
    "#obtaining synsets\n",
    "synsets = wordnet.synsets(word)\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('win.v.01.win')]\n"
     ]
    }
   ],
   "source": [
    "#let's pick win as verb present at 3 position\n",
    "woi = synsets[2];\n",
    "\n",
    "#obtaining lemmas\n",
    "lemmas_win = woi.lemmas();\n",
    "print(lemmas_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['win', 'winnings', 'win', 'profits', 'win', 'acquire', 'win', 'gain', 'gain', 'advance', 'win', 'pull_ahead', 'make_headway', 'get_ahead', 'gain_ground', 'succeed', 'win', 'come_through', 'bring_home_the_bacon', 'deliver_the_goods']\n"
     ]
    }
   ],
   "source": [
    "#obtaining synonyms\n",
    "synonymsArray = [];\n",
    "\n",
    "for syn in synsets: #traversing all synsets of 'win'\n",
    "    for lem in syn.lemmas(): #traversing all lemmas obtained for each synsets\n",
    "        synonymsArray.append(lem.name()); #using name() method to obtain name\n",
    "        \n",
    "print(synonymsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['losings', 'lose', 'lose', 'fall_back', 'fail']\n"
     ]
    }
   ],
   "source": [
    "#obtaining antonyms\n",
    "antonymsArray = [];\n",
    "\n",
    "for syn in synsets:\n",
    "    for lem in syn.lemmas():\n",
    "        for antonyms in lem.antonyms():\n",
    "            antonymsArray.append(antonyms.name());\n",
    "            \n",
    "print(antonymsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('win.n.01'), Synset('winnings.n.01'), Synset('win.v.01'), Synset('acquire.v.05'), Synset('gain.v.05'), Synset('succeed.v.01')]\n",
      "[Lemma('win.v.01.win')]\n",
      "[Lemma('lose.v.02.lose')]\n",
      "lose\n",
      "win.v.01\n"
     ]
    }
   ],
   "source": [
    "#description: how antonyms are obtained?\n",
    "print(synsets)\n",
    "syn = synsets[2];\n",
    "\n",
    "print(syn.lemmas());\n",
    "lemma = syn.lemmas()[0];\n",
    "\n",
    "print(lemma.antonyms());\n",
    "antonym = lemma.antonyms()[0].name();\n",
    "\n",
    "print(antonym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Difference between Lemma and Synset</h4>\n",
    "<b>Lemma: </b>A word in canonical form, with a single meaning\n",
    "<br>\n",
    "<b>Synset: </b> A set of synonyms is a set of words with similar meaning or represent the set of different senses of a particular word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>WUP similarity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It finds similarity based on shortest path to similary hypernyms.\n",
    "- If two words are co-hypernyms then they are much similar then they are children      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cake.n.01'), Synset('patty.n.01'), Synset('cake.n.03'), Synset('coat.v.03')]\n",
      "[Synset('loaf_of_bread.n.01'), Synset('loaf.n.02'), Synset('bum.v.02'), Synset('loiter.v.01')]\n",
      "[Synset('bread.n.01'), Synset('boodle.n.01'), Synset('bread.v.01')]\n"
     ]
    }
   ],
   "source": [
    "w1 = 'cake';\n",
    "w2 = 'loaf';\n",
    "w3 = 'bread';\n",
    "\n",
    "print(wordnet.synsets(w1));\n",
    "cake = wordnet.synsets(w1)[0];\n",
    "\n",
    "print(wordnet.synsets(w2));\n",
    "loafb = wordnet.synsets(w2)[0];\n",
    "loaf  = wordnet.synsets(w2)[1];\n",
    "\n",
    "print(wordnet.synsets(w3));\n",
    "bread = wordnet.synsets(w3)[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wup_similarities are: \n",
      "bread & loaf:  0.7692307692307693\n",
      "bread & loafb:  0.9411764705882353\n",
      "loaf & loafb:  0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print('wup_similarities are: ')\n",
    "print('bread & loaf: ', bread.wup_similarity(loaf))\n",
    "print('bread & loafb: ', bread.wup_similarity(loafb))\n",
    "print('loaf & loafb: ', loaf.wup_similarity(loafb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>How wup_similarity is calculated, overview:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('food.n.02')]\n"
     ]
    }
   ],
   "source": [
    "#let's find hypernym of loaf\n",
    "print(loaf.hypernyms()) #loaf has only one hypernym\n",
    "ref = loaf.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#now we'll find shortest path distance of each word with this ref of hypernyms\n",
    "\n",
    "print(loaf.shortest_path_distance(ref))\n",
    "print(bread.shortest_path_distance(ref))\n",
    "print(loafb.shortest_path_distance(ref))\n",
    "print(cake.shortest_path_distance(ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that:\n",
    "- loaf is 1 path away from hypernym\n",
    "- bread is 2 path away from hypernym\n",
    "- loafb is 3 path away from hypernym\n",
    "- cake is 8 path away from hypernym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Path and LCH Similarities</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decontraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class Replacer():\n",
    "    patterns = [\n",
    "        ('won\\'t', 'will not'), ('\\'d', 'would'), ()\n",
    "    ]\n",
    "    __init__(self):\n",
    "        pass;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
