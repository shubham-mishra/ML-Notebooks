{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decontraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontraction(text):\n",
    "    \"\"\"\n",
    "    function to perform decontraction on provided text\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "    ('won\\'t', 'will not'), ('\\'d', ' would'), ('\\'s', ' is'), ('can\\'t', 'can not'),\n",
    "    ('don\\'t', 'do not'), ('\\'ll', ' will'), ('\\'ve', ' have'), ('\\'t', ' not'),\n",
    "    ('\\'re', ' are'), ('\\'m', ' am')\n",
    "    ]\n",
    "\n",
    "    for (pattern, replacer) in patterns:\n",
    "        regex = re.compile(pattern)\n",
    "        text = regex.sub(replacer, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Function to lemmatize words in provided sentence\n",
    "    \"\"\"\n",
    "    #getting pos for each word in a sentence\n",
    "    pos = nltk.pos_tag(text.split())\n",
    "    \n",
    "    #list to store lemmatize words\n",
    "    lemmatize_words = [];\n",
    "    \n",
    "    #initializing WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #lemmatizing each word in a sentence according to it's pos\n",
    "    for word_pos in pos:\n",
    "        lemmatize_words.append(lemmatizer.lemmatize(word_pos[0], self.get_wordnet_pos(word_pos[1]))) \n",
    "\n",
    "    return ' '.join(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    \"\"\"\n",
    "    Function to perform Stemming\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    function to remove html tags from provided text/sentence\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    \"\"\"\n",
    "    Function to remove special characters.\n",
    "    Parameters: text, remove_digits\n",
    "    remove_digits: To remove digits also, by default it's False\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    if remove_digits:\n",
    "        regex = re.compile(r'[^a-z\\s]')\n",
    "    else:\n",
    "        regex = re.compile(r'[^a-z0-9\\s]')\n",
    "    \n",
    "    return regex.sub('', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(text, words_to_remove_from_stopwords=[]):\n",
    "    \"\"\"\n",
    "    Function to remove stop words from given text/sentence\n",
    "    parameters: text, words_to_remove_from_stopwords\n",
    "    words_to_remove_from_stopwords = list of words to remove from stopwords list\n",
    "    \"\"\"\n",
    "    if len(words_to_remove_from_stopwords) == 0:\n",
    "        for word in words_to_remove_from_stopwords:\n",
    "            STOP_WORDS.remove(word)\n",
    "    \n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in STOP_WORDS])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Roman Numbers\n",
    "not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_roman_number(text):\n",
    "    \"\"\"\n",
    "    function to remove roman numbers\n",
    "    \"\"\"\n",
    "    ProgRoman = re.compile(u'M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$')\n",
    "    t = ProgRoman.sub(' ', text)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Accent Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_accent_characters(text):\n",
    "    \"\"\"\n",
    "    function to replace accent characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_words(text, no_of_words=2):\n",
    "    \"\"\"\n",
    "    function to find most common words in a given text\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    words = [word.lower() for word in text.split()]\n",
    "    counter.update(words)\n",
    "    return counter.most_common(no_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text, words):\n",
    "    \"\"\"\n",
    "    function to remove provided words from given text\n",
    "    \"\"\"\n",
    "    text = ' '.join([word for word in text.split() if word.lower not in words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_breaks(text):\n",
    "    \"\"\"\n",
    "    function to remove line breaks\n",
    "    \"\"\"\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\\"', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finding Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "the geometry\n",
      "the edges\n",
      "the images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"We try to explicitly describe the geometry of the edges of the images.\")\n",
    "\n",
    "for np in doc.noun_chunks: # use np instead of np.text\n",
    "    print(np)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
